{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "from dataloader import Gen_Data_loader, Dis_dataloader\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "from rollout import ROLLOUT\n",
    "# We don't need target_lstm.py, target_lstm can only make random sequence data by RNN, but our target is real domain name data.\n",
    "#from target_lstm import TARGET_LSTM\n",
    "#import domain_translate\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Generator  Hyper-parameters\n",
    "######################################################################################\n",
    "EMB_DIM = 32 # embedding dimension\n",
    "HIDDEN_DIM = 32 # hidden state dimension of lstm cell\n",
    "SEQ_LENGTH = 32 # sequence length\n",
    "START_TOKEN = 0\n",
    "PRE_EPOCH_NUM = 120 # supervise (maximum likelihood estimation) epochs (預設為120)\n",
    "SEED = 88\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Discriminator  Hyper-parameters\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "PRETRAIN_DIS_NUM = 50 #pre-train discriminator times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "#  Basic Training Parameters\n",
    "#########################################################################################\n",
    "TOTAL_BATCH = 200\n",
    "positive_file = ('../Dataset/AlexaTop100K_Separated_Digital/'\n",
    "                  +'TopDomainName.Less33.Separated.Digital-ALL-OF-100000.txt') # 預設值為 'save/real_data.txt'\n",
    "output_path = 'save/' #預設值為 'save/generator_sample.txt' 更改為會更變\n",
    "#eval_file = 'save/eval_file.txt'\n",
    "generated_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_samples(sess, trainable_model, batch_size, generated_num, output_file):\n",
    "    Digit2Real = False\n",
    "    # Generate Samples\n",
    "    generated_samples = []\n",
    "    for _ in range(int(generated_num / batch_size)):\n",
    "        generated_samples.extend(trainable_model.generate(sess))\n",
    "\n",
    "    with open(output_file, 'w') as fout:\n",
    "        for poem in generated_samples:\n",
    "            buffer = ' '.join([str(x) for x in poem]) + '\\n'\n",
    "            fout.write(buffer)\n",
    "            \n",
    "    if \"adversarial_gen\" in output_file:\n",
    "        now=output_file[37:]\n",
    "        output_file=\"save/adversarial_gen/generator_fake_domain_name_\"+now\n",
    "        Digit2Real = True\n",
    "    elif \"pretrain_gen\" in output_file:\n",
    "        now=output_file[34:]\n",
    "        output_file=\"save/pretrain_gen/generator_fake_domain_name_\"+now\n",
    "        Digit2Real = True\n",
    "    \n",
    "    if Digit2Real == True :\n",
    "        digital_table = [\" \",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\".\",\"-\",\n",
    "                            \"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\n",
    "                            \"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\"_\"]    \n",
    "        with open(output_file, 'w') as fout:\n",
    "            for poem in generated_samples:\n",
    "                buffer = \"\".join([digital_table[int(x)] for x in poem]) + '\\n'\n",
    "                fout.write(buffer)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_loss(sess, target_lstm, data_loader):\n",
    "    # target_loss means the oracle negative log-likelihood tested with the oracle model \"target_lstm\"\n",
    "    # For more details, please see the Section 4 in https://arxiv.org/abs/1609.05473\n",
    "    nll = []\n",
    "    data_loader.reset_pointer()\n",
    "\n",
    "    for it in range(data_loader.num_batch):\n",
    "        batch = data_loader.next_batch()\n",
    "        #print(target_lstm.pretrain_loss)\n",
    "        #print({target_lstm.x: batch})\n",
    "        g_loss = sess.run(target_lstm.pretrain_loss, {target_lstm.x: batch})\n",
    "        nll.append(g_loss)\n",
    "\n",
    "    return np.mean(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_train_epoch(sess, trainable_model, data_loader):\n",
    "    # Pre-train the generator using MLE for one epoch\n",
    "    supervised_g_losses = []\n",
    "    data_loader.reset_pointer()\n",
    "    \n",
    "    for it in range(data_loader.num_batch):\n",
    "    #for it in range(50):\n",
    "        #print(\"pre_train\"+str(it))\n",
    "        if it % int(data_loader.num_batch / 30) == 0:\n",
    "            print(\"pre_train_iteration : {0:6} / {1:6}\".format( (it+1), (data_loader.num_batch) ), end=\"\\r\")\n",
    "        batch = data_loader.next_batch()\n",
    "        _, g_loss = trainable_model.pretrain_step(sess, batch)\n",
    "        supervised_g_losses.append(g_loss)\n",
    "\n",
    "    return np.mean(supervised_g_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "assert START_TOKEN == 0\n",
    "\n",
    "gen_data_loader = Gen_Data_loader(BATCH_SIZE)\n",
    "likelihood_data_loader = Gen_Data_loader(BATCH_SIZE) # For testing\n",
    "vocab_size = 40 #預設5000 \n",
    "dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
    "\n",
    "generator = Generator(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN)\n",
    "#target_params = pickle.load(open('save/target_params_py3.pkl','rb'))\n",
    "#target_lstm = TARGET_LSTM(vocab_size, BATCH_SIZE, EMB_DIM, HIDDEN_DIM, SEQ_LENGTH, START_TOKEN, target_params) # The oracle model\n",
    "\n",
    "discriminator = Discriminator(sequence_length=32, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n",
    "                            filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 參數配置\n",
    "config = tf.ConfigProto()\n",
    "# 使用allow_growth option，剛一開始分配少量的GPU容量，然後按需慢慢的增加\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pre-training...\n",
      "[MLE Epoch]:     1 [Cost Time]:      80.77 secs [ETA]:    9611.66 secs\n",
      "[MLE Epoch]:     2 [Cost Time]:     161.97 secs [ETA]:    9556.28 secs\n",
      "[MLE Epoch]:     3 [Cost Time]:     249.82 secs [ETA]:    9743.10 secs\n",
      "[MLE Epoch]:     4 [Cost Time]:     336.89 secs [ETA]:    9769.84 secs\n",
      "[MLE Epoch]:     5 [Cost Time]:     424.27 secs [ETA]:    9758.28 secs\n",
      "[MLE Epoch]:     6 [Cost Time]:     510.84 secs [ETA]:    9705.87 secs\n",
      "[MLE Epoch]:     7 [Cost Time]:     594.98 secs [ETA]:    9604.71 secs\n",
      "[MLE Epoch]:     8 [Cost Time]:     685.10 secs [ETA]:    9591.39 secs\n",
      "[MLE Epoch]:     9 [Cost Time]:     777.64 secs [ETA]:    9590.84 secs\n",
      "[MLE Epoch]:    10 [Cost Time]:     869.37 secs [ETA]:    9563.05 secs\n",
      "[MLE Epoch]:    11 [Cost Time]:     960.84 secs [ETA]:    9521.10 secs\n",
      "[MLE Epoch]:    12 [Cost Time]:    1048.12 secs [ETA]:    9433.08 secs\n",
      "[MLE Epoch]:    13 [Cost Time]:    1135.20 secs [ETA]:    9343.53 secs\n",
      "[MLE Epoch]:    14 [Cost Time]:    1222.71 secs [ETA]:    9257.68 secs\n",
      "[MLE Epoch]:    15 [Cost Time]:    1309.52 secs [ETA]:    9166.61 secs\n",
      "[MLE Epoch]:    16 [Cost Time]:    1396.12 secs [ETA]:    9074.79 secs\n",
      "[MLE Epoch]:    17 [Cost Time]:    1482.98 secs [ETA]:    8985.10 secs\n",
      "[MLE Epoch]:    18 [Cost Time]:    1568.93 secs [ETA]:    8890.59 secs\n",
      "[MLE Epoch]:    19 [Cost Time]:    1654.99 secs [ETA]:    8797.56 secs\n",
      "[MLE Epoch]:    20 [Cost Time]:    1742.06 secs [ETA]:    8710.32 secs\n",
      "[MLE Epoch]:    21 [Cost Time]:    1830.89 secs [ETA]:    8631.35 secs\n",
      "[MLE Epoch]:    22 [Cost Time]:    1919.83 secs [ETA]:    8551.96 secs\n",
      "[MLE Epoch]:    23 [Cost Time]:    2010.66 secs [ETA]:    8479.72 secs\n",
      "[MLE Epoch]:    24 [Cost Time]:    2097.53 secs [ETA]:    8390.13 secs\n",
      "[MLE Epoch]:    25 [Cost Time]:    2183.79 secs [ETA]:    8298.41 secs\n",
      "[MLE Epoch]:    26 [Cost Time]:    2273.50 secs [ETA]:    8219.57 secs\n",
      "[MLE Epoch]:    27 [Cost Time]:    2362.96 secs [ETA]:    8139.10 secs\n",
      "[MLE Epoch]:    28 [Cost Time]:    2449.48 secs [ETA]:    8048.28 secs\n",
      "[MLE Epoch]:    29 [Cost Time]:    2536.59 secs [ETA]:    7959.63 secs\n",
      "[MLE Epoch]:    30 [Cost Time]:    2621.70 secs [ETA]:    7865.09 secs\n",
      "[MLE Epoch]:    31 [Cost Time]:    2708.67 secs [ETA]:    7776.49 secs\n",
      "[MLE Epoch]:    32 [Cost Time]:    2795.33 secs [ETA]:    7687.16 secs\n",
      "[MLE Epoch]:    33 [Cost Time]:    2882.34 secs [ETA]:    7598.90 secs\n",
      "[MLE Epoch]:    34 [Cost Time]:    2969.39 secs [ETA]:    7510.81 secs\n",
      "[MLE Epoch]:    35 [Cost Time]:    3056.41 secs [ETA]:    7422.71 secs\n",
      "[MLE Epoch]:    36 [Cost Time]:    3145.08 secs [ETA]:    7338.52 secs\n",
      "[MLE Epoch]:    37 [Cost Time]:    3232.60 secs [ETA]:    7251.51 secs\n",
      "[MLE Epoch]:    38 [Cost Time]:    3320.81 secs [ETA]:    7165.97 secs\n",
      "[MLE Epoch]:    39 [Cost Time]:    3408.03 secs [ETA]:    7078.21 secs\n",
      "[MLE Epoch]:    40 [Cost Time]:    3492.19 secs [ETA]:    6984.39 secs\n",
      "[MLE Epoch]:    41 [Cost Time]:    3579.61 secs [ETA]:    6897.29 secs\n",
      "[MLE Epoch]:    42 [Cost Time]:    3665.55 secs [ETA]:    6807.45 secs\n",
      "[MLE Epoch]:    43 [Cost Time]:    3752.02 secs [ETA]:    6718.73 secs\n",
      "[MLE Epoch]:    44 [Cost Time]:    3836.43 secs [ETA]:    6626.56 secs\n",
      "[MLE Epoch]:    45 [Cost Time]:    3922.28 secs [ETA]:    6537.13 secs\n",
      "[MLE Epoch]:    46 [Cost Time]:    4009.07 secs [ETA]:    6449.37 secs\n",
      "[MLE Epoch]:    47 [Cost Time]:    4094.61 secs [ETA]:    6359.72 secs\n",
      "[MLE Epoch]:    48 [Cost Time]:    4180.94 secs [ETA]:    6271.42 secs\n",
      "[MLE Epoch]:    49 [Cost Time]:    4269.10 secs [ETA]:    6185.84 secs\n",
      "[MLE Epoch]:    50 [Cost Time]:    4357.35 secs [ETA]:    6100.29 secs\n",
      "[MLE Epoch]:    51 [Cost Time]:    4445.46 secs [ETA]:    6014.44 secs\n",
      "[MLE Epoch]:    52 [Cost Time]:    4534.37 secs [ETA]:    5929.56 secs\n",
      "[MLE Epoch]:    53 [Cost Time]:    4617.03 secs [ETA]:    5836.62 secs\n",
      "[MLE Epoch]:    54 [Cost Time]:    4698.25 secs [ETA]:    5742.30 secs\n",
      "[MLE Epoch]:    55 [Cost Time]:    4780.16 secs [ETA]:    5649.27 secs\n",
      "[MLE Epoch]:    56 [Cost Time]:    4859.52 secs [ETA]:    5553.74 secs\n",
      "[MLE Epoch]:    57 [Cost Time]:    4940.62 secs [ETA]:    5460.68 secs\n",
      "[MLE Epoch]:    58 [Cost Time]:    5021.76 secs [ETA]:    5368.08 secs\n",
      "[MLE Epoch]:    59 [Cost Time]:    5103.65 secs [ETA]:    5276.65 secs\n",
      "[MLE Epoch]:    60 [Cost Time]:    5183.62 secs [ETA]:    5183.62 secs\n",
      "[MLE Epoch]:    61 [Cost Time]:    5266.02 secs [ETA]:    5093.36 secs\n",
      "[MLE Epoch]:    62 [Cost Time]:    5348.48 secs [ETA]:    5003.42 secs\n",
      "[MLE Epoch]:    63 [Cost Time]:    5419.13 secs [ETA]:    4903.02 secs\n",
      "[MLE Epoch]:    64 [Cost Time]:    5489.64 secs [ETA]:    4803.44 secs\n",
      "[MLE Epoch]:    65 [Cost Time]:    5560.60 secs [ETA]:    4705.12 secs\n",
      "[MLE Epoch]:    66 [Cost Time]:    5631.26 secs [ETA]:    4607.39 secs\n",
      "[MLE Epoch]:    67 [Cost Time]:    5702.07 secs [ETA]:    4510.59 secs\n",
      "[MLE Epoch]:    68 [Cost Time]:    5772.79 secs [ETA]:    4414.49 secs\n",
      "[MLE Epoch]:    69 [Cost Time]:    5843.19 secs [ETA]:    4318.88 secs\n",
      "[MLE Epoch]:    70 [Cost Time]:    5914.15 secs [ETA]:    4224.39 secs\n",
      "[MLE Epoch]:    71 [Cost Time]:    5984.89 secs [ETA]:    4130.42 secs\n",
      "[MLE Epoch]:    72 [Cost Time]:    6055.77 secs [ETA]:    4037.18 secs\n",
      "[MLE Epoch]:    73 [Cost Time]:    6126.24 secs [ETA]:    3944.29 secs\n",
      "[MLE Epoch]:    74 [Cost Time]:    6196.67 secs [ETA]:    3851.98 secs\n",
      "[MLE Epoch]:    75 [Cost Time]:    6267.21 secs [ETA]:    3760.32 secs\n",
      "[MLE Epoch]:    76 [Cost Time]:    6338.00 secs [ETA]:    3669.37 secs\n",
      "[MLE Epoch]:    77 [Cost Time]:    6408.68 secs [ETA]:    3578.87 secs\n",
      "[MLE Epoch]:    78 [Cost Time]:    6479.82 secs [ETA]:    3489.13 secs\n",
      "[MLE Epoch]:    79 [Cost Time]:    6550.78 secs [ETA]:    3399.77 secs\n",
      "[MLE Epoch]:    80 [Cost Time]:    6621.36 secs [ETA]:    3310.68 secs\n",
      "[MLE Epoch]:    81 [Cost Time]:    6691.83 secs [ETA]:    3221.99 secs\n",
      "[MLE Epoch]:    82 [Cost Time]:    6763.35 secs [ETA]:    3134.24 secs\n",
      "[MLE Epoch]:    83 [Cost Time]:    6833.87 secs [ETA]:    3046.42 secs\n",
      "[MLE Epoch]:    84 [Cost Time]:    6904.51 secs [ETA]:    2959.08 secs\n",
      "[MLE Epoch]:    85 [Cost Time]:    6975.40 secs [ETA]:    2872.22 secs\n",
      "[MLE Epoch]:    86 [Cost Time]:    7046.02 secs [ETA]:    2785.64 secs\n",
      "[MLE Epoch]:    87 [Cost Time]:    7116.79 secs [ETA]:    2699.47 secs\n",
      "[MLE Epoch]:    88 [Cost Time]:    7187.15 secs [ETA]:    2613.51 secs\n",
      "[MLE Epoch]:    89 [Cost Time]:    7257.87 secs [ETA]:    2528.02 secs\n",
      "[MLE Epoch]:    90 [Cost Time]:    7328.24 secs [ETA]:    2442.75 secs\n",
      "[MLE Epoch]:    91 [Cost Time]:    7398.91 secs [ETA]:    2357.89 secs\n",
      "[MLE Epoch]:    92 [Cost Time]:    7469.40 secs [ETA]:    2273.30 secs\n",
      "[MLE Epoch]:    93 [Cost Time]:    7539.77 secs [ETA]:    2188.97 secs\n",
      "[MLE Epoch]:    94 [Cost Time]:    7610.46 secs [ETA]:    2105.02 secs\n",
      "[MLE Epoch]:    95 [Cost Time]:    7681.20 secs [ETA]:    2021.37 secs\n",
      "[MLE Epoch]:    96 [Cost Time]:    7752.01 secs [ETA]:    1938.00 secs\n",
      "[MLE Epoch]:    97 [Cost Time]:    7822.41 secs [ETA]:    1854.80 secs\n",
      "[MLE Epoch]:    98 [Cost Time]:    7893.22 secs [ETA]:    1771.95 secs\n",
      "[MLE Epoch]:    99 [Cost Time]:    7963.71 secs [ETA]:    1689.27 secs\n",
      "[MLE Epoch]:   100 [Cost Time]:    8034.02 secs [ETA]:    1606.80 secs\n",
      "[MLE Epoch]:   101 [Cost Time]:    8104.75 secs [ETA]:    1524.66 secs\n",
      "[MLE Epoch]:   102 [Cost Time]:    8175.33 secs [ETA]:    1442.71 secs\n",
      "[MLE Epoch]:   103 [Cost Time]:    8246.21 secs [ETA]:    1361.02 secs\n",
      "[MLE Epoch]:   104 [Cost Time]:    8316.94 secs [ETA]:    1279.53 secs\n",
      "[MLE Epoch]:   105 [Cost Time]:    8387.50 secs [ETA]:    1198.21 secs\n",
      "[MLE Epoch]:   106 [Cost Time]:    8458.14 secs [ETA]:    1117.11 secs\n",
      "[MLE Epoch]:   107 [Cost Time]:    8528.59 secs [ETA]:    1036.18 secs\n",
      "[MLE Epoch]:   108 [Cost Time]:    8599.26 secs [ETA]:     955.47 secs\n",
      "[MLE Epoch]:   109 [Cost Time]:    8669.81 secs [ETA]:     874.93 secs\n",
      "[MLE Epoch]:   110 [Cost Time]:    8740.22 secs [ETA]:     794.57 secs\n",
      "[MLE Epoch]:   111 [Cost Time]:    8810.81 secs [ETA]:     714.39 secs\n",
      "[MLE Epoch]:   112 [Cost Time]:    8881.29 secs [ETA]:     634.38 secs\n",
      "[MLE Epoch]:   113 [Cost Time]:    8952.62 secs [ETA]:     554.59 secs\n",
      "[MLE Epoch]:   114 [Cost Time]:    9022.99 secs [ETA]:     474.89 secs\n",
      "[MLE Epoch]:   115 [Cost Time]:    9093.57 secs [ETA]:     395.37 secs\n",
      "[MLE Epoch]:   116 [Cost Time]:    9164.19 secs [ETA]:     316.01 secs\n",
      "[MLE Epoch]:   117 [Cost Time]:    9234.63 secs [ETA]:     236.79 secs\n",
      "[MLE Epoch]:   118 [Cost Time]:    9305.43 secs [ETA]:     157.72 secs\n",
      "[MLE Epoch]:   119 [Cost Time]:    9375.81 secs [ETA]:      78.79 secs\n",
      "[MLE Epoch]:   120 [Cost Time]:    9446.68 secs [ETA]:       0.00 secs\n"
     ]
    }
   ],
   "source": [
    "# First, use the oracle model to provide the positive examples, which are sampled from the oracle data distribution\n",
    "#generate_samples(sess, target_lstm, BATCH_SIZE, generated_num, positive_file)\n",
    "gen_data_loader.create_batches(positive_file)\n",
    "\n",
    "log = open('save/experiment-log.txt', 'w')\n",
    "#  pre-train generator\n",
    "print('Start pre-training...')\n",
    "log.write('pre-training...\\n')\n",
    "now_time = time.clock()\n",
    "sum_time = 0.\n",
    "for epoch in range(PRE_EPOCH_NUM):\n",
    "    loss = pre_train_epoch(sess, generator, gen_data_loader)    \n",
    "    \n",
    "    eval_file = ( output_path + \"pretrain_gen/generator_digit_{0}.txt\").format(str(epoch+1).zfill(2))\n",
    "    generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
    "    likelihood_data_loader.create_batches(eval_file)\n",
    "    \n",
    "    '''test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
    "    print('pre-train epoch ', epoch, 'test_loss ', test_loss)\n",
    "    buffer = 'epoch:\\t'+ str(epoch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "    log.write(buffer)'''\n",
    "    after_time=time.clock() - now_time\n",
    "    eta_time = (after_time / (epoch+1) )*(PRE_EPOCH_NUM-(epoch+1))\n",
    "    print(\"[MLE Epoch]: {0:5} [Cost Time]: {1:10.2f} secs [ETA]: {2:10.2f} secs\".format( (epoch+1), after_time, eta_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start pre-training discriminator...\n",
      "[Pre-train Discriminator Epoch]:     1 [Cost Time]:      83.75 secs [ETA]:    4103.51 secs\n",
      "[Pre-train Discriminator Epoch]:     2 [Cost Time]:     161.27 secs [ETA]:    3870.43 secs\n",
      "[Pre-train Discriminator Epoch]:     3 [Cost Time]:     238.52 secs [ETA]:    3736.76 secs\n",
      "[Pre-train Discriminator Epoch]:     4 [Cost Time]:     316.01 secs [ETA]:    3634.08 secs\n",
      "[Pre-train Discriminator Epoch]:     5 [Cost Time]:     393.78 secs [ETA]:    3543.99 secs\n",
      "[Pre-train Discriminator Epoch]:     6 [Cost Time]:     471.38 secs [ETA]:    3456.77 secs\n",
      "[Pre-train Discriminator Epoch]:     7 [Cost Time]:     548.61 secs [ETA]:    3370.06 secs\n",
      "[Pre-train Discriminator Epoch]:     8 [Cost Time]:     626.00 secs [ETA]:    3286.49 secs\n",
      "[Pre-train Discriminator Epoch]:     9 [Cost Time]:     703.34 secs [ETA]:    3204.10 secs\n",
      "[Pre-train Discriminator Epoch]:    10 [Cost Time]:     780.53 secs [ETA]:    3122.10 secs\n",
      "[Pre-train Discriminator Epoch]:    11 [Cost Time]:     858.67 secs [ETA]:    3044.37 secs\n",
      "[Pre-train Discriminator Epoch]:    12 [Cost Time]:     936.16 secs [ETA]:    2964.49 secs\n",
      "[Pre-train Discriminator Epoch]:    13 [Cost Time]:    1013.46 secs [ETA]:    2884.45 secs\n",
      "[Pre-train Discriminator Epoch]:    14 [Cost Time]:    1090.76 secs [ETA]:    2804.81 secs\n",
      "[Pre-train Discriminator Epoch]:    15 [Cost Time]:    1167.96 secs [ETA]:    2725.24 secs\n",
      "[Pre-train Discriminator Epoch]:    16 [Cost Time]:    1245.21 secs [ETA]:    2646.06 secs\n",
      "[Pre-train Discriminator Epoch]:    17 [Cost Time]:    1322.70 secs [ETA]:    2567.60 secs\n",
      "[Pre-train Discriminator Epoch]:    18 [Cost Time]:    1400.04 secs [ETA]:    2488.96 secs\n",
      "[Pre-train Discriminator Epoch]:    19 [Cost Time]:    1477.19 secs [ETA]:    2410.16 secs\n",
      "[Pre-train Discriminator Epoch]:    20 [Cost Time]:    1554.53 secs [ETA]:    2331.79 secs\n",
      "[Pre-train Discriminator Epoch]:    21 [Cost Time]:    1631.73 secs [ETA]:    2253.34 secs\n",
      "[Pre-train Discriminator Epoch]:    22 [Cost Time]:    1708.93 secs [ETA]:    2175.00 secs\n",
      "[Pre-train Discriminator Epoch]:    23 [Cost Time]:    1786.45 secs [ETA]:    2097.14 secs\n",
      "[Pre-train Discriminator Epoch]:    24 [Cost Time]:    1864.04 secs [ETA]:    2019.38 secs\n",
      "[Pre-train Discriminator Epoch]:    25 [Cost Time]:    1941.18 secs [ETA]:    1941.18 secs\n",
      "[Pre-train Discriminator Epoch]:    26 [Cost Time]:    2018.47 secs [ETA]:    1863.20 secs\n",
      "[Pre-train Discriminator Epoch]:    27 [Cost Time]:    2095.71 secs [ETA]:    1785.23 secs\n",
      "[Pre-train Discriminator Epoch]:    28 [Cost Time]:    2172.91 secs [ETA]:    1707.29 secs\n",
      "[Pre-train Discriminator Epoch]:    29 [Cost Time]:    2250.44 secs [ETA]:    1629.63 secs\n",
      "[Pre-train Discriminator Epoch]:    30 [Cost Time]:    2328.00 secs [ETA]:    1552.00 secs\n",
      "[Pre-train Discriminator Epoch]:    31 [Cost Time]:    2405.27 secs [ETA]:    1474.20 secs\n",
      "[Pre-train Discriminator Epoch]:    32 [Cost Time]:    2482.50 secs [ETA]:    1396.40 secs\n",
      "[Pre-train Discriminator Epoch]:    33 [Cost Time]:    2559.85 secs [ETA]:    1318.71 secs\n",
      "[Pre-train Discriminator Epoch]:    34 [Cost Time]:    2637.08 secs [ETA]:    1240.98 secs\n",
      "[Pre-train Discriminator Epoch]:    35 [Cost Time]:    2714.54 secs [ETA]:    1163.38 secs\n",
      "[Pre-train Discriminator Epoch]:    36 [Cost Time]:    2791.95 secs [ETA]:    1085.76 secs\n",
      "[Pre-train Discriminator Epoch]:    37 [Cost Time]:    2869.49 secs [ETA]:    1008.20 secs\n",
      "[Pre-train Discriminator Epoch]:    38 [Cost Time]:    2946.71 secs [ETA]:     930.54 secs\n",
      "[Pre-train Discriminator Epoch]:    39 [Cost Time]:    3023.88 secs [ETA]:     852.89 secs\n",
      "[Pre-train Discriminator Epoch]:    40 [Cost Time]:    3101.17 secs [ETA]:     775.29 secs\n",
      "[Pre-train Discriminator Epoch]:    41 [Cost Time]:    3178.73 secs [ETA]:     697.77 secs\n",
      "[Pre-train Discriminator Epoch]:    42 [Cost Time]:    3256.16 secs [ETA]:     620.22 secs\n",
      "[Pre-train Discriminator Epoch]:    43 [Cost Time]:    3333.94 secs [ETA]:     542.73 secs\n",
      "[Pre-train Discriminator Epoch]:    44 [Cost Time]:    3411.19 secs [ETA]:     465.16 secs\n",
      "[Pre-train Discriminator Epoch]:    45 [Cost Time]:    3488.42 secs [ETA]:     387.60 secs\n",
      "[Pre-train Discriminator Epoch]:    46 [Cost Time]:    3565.57 secs [ETA]:     310.05 secs\n",
      "[Pre-train Discriminator Epoch]:    47 [Cost Time]:    3643.08 secs [ETA]:     232.54 secs\n",
      "[Pre-train Discriminator Epoch]:    48 [Cost Time]:    3720.56 secs [ETA]:     155.02 secs\n",
      "[Pre-train Discriminator Epoch]:    49 [Cost Time]:    3798.08 secs [ETA]:      77.51 secs\n",
      "[Pre-train Discriminator Epoch]:    50 [Cost Time]:    3875.34 secs [ETA]:       0.00 secs\n"
     ]
    }
   ],
   "source": [
    "print('Start pre-training discriminator...')\n",
    "# Train 3 epoch on the generated data and do this for 50 times\n",
    "now_time = time.clock()\n",
    "sum_time = 0.\n",
    "pre_D_epoch = 0\n",
    "for _ in range(PRETRAIN_DIS_NUM): #50\n",
    "    pretrain_D = ( output_path + \"pretrain_discriminator.txt\")\n",
    "    generate_samples(sess, generator, BATCH_SIZE, generated_num, pretrain_D)\n",
    "    dis_data_loader.load_train_data(positive_file, pretrain_D)\n",
    "    for _ in range(3):\n",
    "        dis_data_loader.reset_pointer()\n",
    "        for it in range(dis_data_loader.num_batch):\n",
    "            if it % int(dis_data_loader.num_batch / 30) == 0:\n",
    "                print(\"pre_train_iteration : {0:6} / {1:6}\".format( (it+1), (dis_data_loader.num_batch) ), end=\"\\r\")\n",
    "            x_batch, y_batch = dis_data_loader.next_batch()\n",
    "            feed = {\n",
    "                discriminator.input_x: x_batch,\n",
    "                discriminator.input_y: y_batch,\n",
    "                discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "            }\n",
    "            _ = sess.run(discriminator.train_op, feed)\n",
    "    after_time=time.clock() - now_time\n",
    "    eta_time = (after_time / (pre_D_epoch+1) )*(PRETRAIN_DIS_NUM-(pre_D_epoch+1))\n",
    "    print(\"[Pre-train Discriminator Epoch]: {0:5} [Cost Time]: {1:10.2f} secs [ETA]: {2:10.2f} secs\".format( (pre_D_epoch+1), after_time, eta_time))\n",
    "    pre_D_epoch = pre_D_epoch +1\n",
    "\n",
    "rollout = ROLLOUT(generator, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################################################################\n",
      "Start Adversarial Training...\n",
      "[GAN Epoch]:     1 [Cost Time]:     400.09 secs [ETA]:   79618.07 secs\n",
      "[GAN Epoch]:     2 [Cost Time]:     797.70 secs [ETA]:   78972.08 secs\n",
      "[GAN Epoch]:     3 [Cost Time]:    1196.93 secs [ETA]:   78598.21 secs\n",
      "[GAN Epoch]:     4 [Cost Time]:    1596.08 secs [ETA]:   78207.78 secs\n",
      "[GAN Epoch]:     5 [Cost Time]:    1992.76 secs [ETA]:   77717.54 secs\n",
      "[GAN Epoch]:     6 [Cost Time]:    2390.40 secs [ETA]:   77289.69 secs\n",
      "[GAN Epoch]:     7 [Cost Time]:    2789.96 secs [ETA]:   76923.06 secs\n",
      "[GAN Epoch]:     8 [Cost Time]:    3189.88 secs [ETA]:   76557.04 secs\n",
      "[GAN Epoch]:     9 [Cost Time]:    3589.48 secs [ETA]:   76176.70 secs\n",
      "[GAN Epoch]:    10 [Cost Time]:    3988.43 secs [ETA]:   75780.25 secs\n",
      "[GAN Epoch]:    11 [Cost Time]:    4387.18 secs [ETA]:   75379.64 secs\n",
      "[GAN Epoch]:    12 [Cost Time]:    4785.76 secs [ETA]:   74976.83 secs\n",
      "[GAN Epoch]:    13 [Cost Time]:    5183.69 secs [ETA]:   74565.34 secs\n",
      "[GAN Epoch]:    14 [Cost Time]:    5583.18 secs [ETA]:   74176.59 secs\n",
      "[GAN Epoch]:    15 [Cost Time]:    5981.83 secs [ETA]:   73775.91 secs\n",
      "[GAN Epoch]:    16 [Cost Time]:    6382.79 secs [ETA]:   73402.06 secs\n",
      "[GAN Epoch]:    17 [Cost Time]:    6782.86 secs [ETA]:   73015.49 secs\n",
      "[GAN Epoch]:    18 [Cost Time]:    7182.49 secs [ETA]:   72622.97 secs\n",
      "[GAN Epoch]:    19 [Cost Time]:    7581.13 secs [ETA]:   72220.28 secs\n",
      "[GAN Epoch]:    20 [Cost Time]:    7980.21 secs [ETA]:   71821.92 secs\n",
      "[GAN Epoch]:    21 [Cost Time]:    8376.34 secs [ETA]:   71398.32 secs\n",
      "[GAN Epoch]:    22 [Cost Time]:    8775.30 secs [ETA]:   71000.15 secs\n",
      "[GAN Epoch]:    23 [Cost Time]:    9173.72 secs [ETA]:   70597.73 secs\n",
      "[GAN Epoch]:    24 [Cost Time]:    9572.78 secs [ETA]:   70200.36 secs\n",
      "[GAN Epoch]:    25 [Cost Time]:    9969.54 secs [ETA]:   69786.75 secs\n",
      "[GAN Epoch]:    26 [Cost Time]:   10369.05 secs [ETA]:   69392.89 secs\n",
      "[GAN Epoch]:    27 [Cost Time]:   10767.56 secs [ETA]:   68992.13 secs\n",
      "[GAN Epoch]:    28 [Cost Time]:   11162.58 secs [ETA]:   68570.13 secs\n",
      "[GAN Epoch]:    29 [Cost Time]:   11560.33 secs [ETA]:   68166.06 secs\n",
      "[GAN Epoch]:    30 [Cost Time]:   11959.22 secs [ETA]:   67768.93 secs\n",
      "[GAN Epoch]:    31 [Cost Time]:   12357.31 secs [ETA]:   67367.29 secs\n",
      "[GAN Epoch]:    32 [Cost Time]:   12757.87 secs [ETA]:   66978.81 secs\n",
      "[GAN Epoch]:    33 [Cost Time]:   13156.43 secs [ETA]:   66579.51 secs\n",
      "[GAN Epoch]:    34 [Cost Time]:   13556.82 secs [ETA]:   66189.17 secs\n",
      "[GAN Epoch]:    35 [Cost Time]:   13955.91 secs [ETA]:   65792.17 secs\n",
      "[GAN Epoch]:    36 [Cost Time]:   14356.45 secs [ETA]:   65401.59 secs\n",
      "[GAN Epoch]:    37 [Cost Time]:   14756.90 secs [ETA]:   65010.14 secs\n",
      "[GAN Epoch]:    38 [Cost Time]:   15157.84 secs [ETA]:   64620.25 secs\n",
      "[GAN Epoch]:    39 [Cost Time]:   15556.37 secs [ETA]:   64219.87 secs\n",
      "[GAN Epoch]:    40 [Cost Time]:   15954.92 secs [ETA]:   63819.67 secs\n",
      "[GAN Epoch]:    41 [Cost Time]:   16352.94 secs [ETA]:   63417.51 secs\n",
      "[GAN Epoch]:    42 [Cost Time]:   16750.81 secs [ETA]:   63014.95 secs\n",
      "[GAN Epoch]:    43 [Cost Time]:   17169.51 secs [ETA]:   62688.66 secs\n",
      "[GAN Epoch]:    44 [Cost Time]:   17599.97 secs [ETA]:   62399.90 secs\n",
      "[GAN Epoch]:    45 [Cost Time]:   18029.70 secs [ETA]:   62102.31 secs\n",
      "[GAN Epoch]:    46 [Cost Time]:   18453.05 secs [ETA]:   61777.60 secs\n",
      "[GAN Epoch]:    47 [Cost Time]:   18850.92 secs [ETA]:   61365.76 secs\n",
      "[GAN Epoch]:    48 [Cost Time]:   19248.16 secs [ETA]:   60952.51 secs\n",
      "[GAN Epoch]:    49 [Cost Time]:   19645.25 secs [ETA]:   60539.46 secs\n",
      "[GAN Epoch]:    50 [Cost Time]:   20042.42 secs [ETA]:   60127.26 secs\n",
      "[GAN Epoch]:    51 [Cost Time]:   20439.54 secs [ETA]:   59715.53 secs\n",
      "[GAN Epoch]:    52 [Cost Time]:   20838.76 secs [ETA]:   59310.31 secs\n",
      "[GAN Epoch]:    53 [Cost Time]:   21235.91 secs [ETA]:   58899.59 secs\n",
      "[GAN Epoch]:    54 [Cost Time]:   21634.99 secs [ETA]:   58494.60 secs\n",
      "[GAN Epoch]:    55 [Cost Time]:   22031.82 secs [ETA]:   58083.90 secs\n",
      "[GAN Epoch]:    56 [Cost Time]:   22428.91 secs [ETA]:   57674.34 secs\n",
      "[GAN Epoch]:    57 [Cost Time]:   22825.94 secs [ETA]:   57265.07 secs\n",
      "[GAN Epoch]:    58 [Cost Time]:   23223.13 secs [ETA]:   56856.63 secs\n",
      "[GAN Epoch]:    59 [Cost Time]:   23620.63 secs [ETA]:   56449.30 secs\n",
      "[GAN Epoch]:    60 [Cost Time]:   24018.09 secs [ETA]:   56042.21 secs\n",
      "[GAN Epoch]:    61 [Cost Time]:   24415.39 secs [ETA]:   55635.08 secs\n",
      "[GAN Epoch]:    62 [Cost Time]:   24812.49 secs [ETA]:   55227.80 secs\n",
      "[GAN Epoch]:    63 [Cost Time]:   25209.70 secs [ETA]:   54821.10 secs\n",
      "[GAN Epoch]:    64 [Cost Time]:   25606.86 secs [ETA]:   54414.58 secs\n",
      "[GAN Epoch]:    65 [Cost Time]:   26005.10 secs [ETA]:   54010.58 secs\n",
      "[GAN Epoch]:    66 [Cost Time]:   26404.86 secs [ETA]:   53609.86 secs\n",
      "[GAN Epoch]:    67 [Cost Time]:   26804.06 secs [ETA]:   53208.05 secs\n",
      "[GAN Epoch]:    68 [Cost Time]:   27201.36 secs [ETA]:   52802.64 secs\n",
      "[GAN Epoch]:    69 [Cost Time]:   27599.84 secs [ETA]:   52399.70 secs\n",
      "[GAN Epoch]:    70 [Cost Time]:   27998.07 secs [ETA]:   51996.42 secs\n",
      "[GAN Epoch]:    71 [Cost Time]:   28397.56 secs [ETA]:   51595.57 secs\n",
      "[GAN Epoch]:    72 [Cost Time]:   28795.33 secs [ETA]:   51191.70 secs\n",
      "[GAN Epoch]:    73 [Cost Time]:   29192.28 secs [ETA]:   50786.57 secs\n",
      "[GAN Epoch]:    74 [Cost Time]:   29589.88 secs [ETA]:   50382.77 secs\n",
      "[GAN Epoch]:    75 [Cost Time]:   29989.66 secs [ETA]:   49982.77 secs\n",
      "[GAN Epoch]:    76 [Cost Time]:   30389.60 secs [ETA]:   49583.03 secs\n",
      "[GAN Epoch]:    77 [Cost Time]:   30789.42 secs [ETA]:   49183.10 secs\n",
      "[GAN Epoch]:    78 [Cost Time]:   31188.15 secs [ETA]:   48781.46 secs\n",
      "[GAN Epoch]:    79 [Cost Time]:   31587.72 secs [ETA]:   48381.19 secs\n",
      "[GAN Epoch]:    80 [Cost Time]:   31986.94 secs [ETA]:   47980.41 secs\n",
      "[GAN Epoch]:    81 [Cost Time]:   32385.40 secs [ETA]:   47578.55 secs\n",
      "[GAN Epoch]:    82 [Cost Time]:   32785.36 secs [ETA]:   47178.94 secs\n",
      "[GAN Epoch]:    83 [Cost Time]:   33184.44 secs [ETA]:   46778.06 secs\n",
      "[GAN Epoch]:    84 [Cost Time]:   33581.98 secs [ETA]:   46375.12 secs\n",
      "[GAN Epoch]:    85 [Cost Time]:   33981.20 secs [ETA]:   45974.57 secs\n",
      "[GAN Epoch]:    86 [Cost Time]:   34380.02 secs [ETA]:   45573.51 secs\n",
      "[GAN Epoch]:    87 [Cost Time]:   34777.63 secs [ETA]:   45170.94 secs\n",
      "[GAN Epoch]:    88 [Cost Time]:   35175.33 secs [ETA]:   44768.61 secs\n",
      "[GAN Epoch]:    89 [Cost Time]:   35573.11 secs [ETA]:   44366.47 secs\n",
      "[GAN Epoch]:    90 [Cost Time]:   35971.04 secs [ETA]:   43964.60 secs\n",
      "[GAN Epoch]:    91 [Cost Time]:   36371.07 secs [ETA]:   43565.34 secs\n",
      "[GAN Epoch]:    92 [Cost Time]:   36771.30 secs [ETA]:   43166.31 secs\n",
      "[GAN Epoch]:    93 [Cost Time]:   37170.64 secs [ETA]:   42766.22 secs\n",
      "[GAN Epoch]:    94 [Cost Time]:   37570.35 secs [ETA]:   42366.57 secs\n",
      "[GAN Epoch]:    95 [Cost Time]:   37971.15 secs [ETA]:   41968.11 secs\n",
      "[GAN Epoch]:    96 [Cost Time]:   38370.91 secs [ETA]:   41568.48 secs\n",
      "[GAN Epoch]:    97 [Cost Time]:   38769.60 secs [ETA]:   41167.72 secs\n",
      "[GAN Epoch]:    98 [Cost Time]:   39167.69 secs [ETA]:   40766.37 secs\n",
      "[GAN Epoch]:    99 [Cost Time]:   39566.63 secs [ETA]:   40365.96 secs\n",
      "[GAN Epoch]:   100 [Cost Time]:   39964.01 secs [ETA]:   39964.01 secs\n",
      "[GAN Epoch]:   101 [Cost Time]:   40363.21 secs [ETA]:   39563.94 secs\n",
      "[GAN Epoch]:   102 [Cost Time]:   40760.79 secs [ETA]:   39162.33 secs\n",
      "[GAN Epoch]:   103 [Cost Time]:   41159.04 secs [ETA]:   38761.43 secs\n",
      "[GAN Epoch]:   104 [Cost Time]:   41556.74 secs [ETA]:   38360.07 secs\n",
      "[GAN Epoch]:   105 [Cost Time]:   41954.16 secs [ETA]:   37958.52 secs\n",
      "[GAN Epoch]:   106 [Cost Time]:   42354.18 secs [ETA]:   37559.37 secs\n",
      "[GAN Epoch]:   107 [Cost Time]:   42752.94 secs [ETA]:   37159.09 secs\n",
      "[GAN Epoch]:   108 [Cost Time]:   43152.67 secs [ETA]:   36759.68 secs\n",
      "[GAN Epoch]:   109 [Cost Time]:   43550.57 secs [ETA]:   36358.73 secs\n",
      "[GAN Epoch]:   110 [Cost Time]:   43948.08 secs [ETA]:   35957.52 secs\n",
      "[GAN Epoch]:   111 [Cost Time]:   44345.46 secs [ETA]:   35556.27 secs\n",
      "[GAN Epoch]:   112 [Cost Time]:   44743.48 secs [ETA]:   35155.59 secs\n",
      "[GAN Epoch]:   113 [Cost Time]:   45141.49 secs [ETA]:   34754.95 secs\n",
      "[GAN Epoch]:   114 [Cost Time]:   45539.69 secs [ETA]:   34354.50 secs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GAN Epoch]:   115 [Cost Time]:   45937.95 secs [ETA]:   33954.14 secs\n",
      "[GAN Epoch]:   116 [Cost Time]:   46336.35 secs [ETA]:   33553.91 secs\n",
      "[GAN Epoch]:   117 [Cost Time]:   46734.05 secs [ETA]:   33153.22 secs\n",
      "[GAN Epoch]:   118 [Cost Time]:   47133.04 secs [ETA]:   32753.47 secs\n",
      "[GAN Epoch]:   119 [Cost Time]:   47531.36 secs [ETA]:   32353.28 secs\n",
      "[GAN Epoch]:   120 [Cost Time]:   47930.61 secs [ETA]:   31953.74 secs\n",
      "[GAN Epoch]:   121 [Cost Time]:   48328.28 secs [ETA]:   31553.17 secs\n",
      "[GAN Epoch]:   122 [Cost Time]:   48728.86 secs [ETA]:   31154.52 secs\n",
      "[GAN Epoch]:   123 [Cost Time]:   49128.45 secs [ETA]:   30755.21 secs\n",
      "[GAN Epoch]:   124 [Cost Time]:   49526.74 secs [ETA]:   30355.10 secs\n",
      "[GAN Epoch]:   125 [Cost Time]:   49925.23 secs [ETA]:   29955.14 secs\n",
      "[GAN Epoch]:   126 [Cost Time]:   50324.98 secs [ETA]:   29555.94 secs\n",
      "[GAN Epoch]:   127 [Cost Time]:   50725.54 secs [ETA]:   29157.20 secs\n",
      "[GAN Epoch]:   128 [Cost Time]:   51124.51 secs [ETA]:   28757.54 secs\n",
      "[GAN Epoch]:   129 [Cost Time]:   51522.66 secs [ETA]:   28357.43 secs\n",
      "[GAN Epoch]:   130 [Cost Time]:   51921.07 secs [ETA]:   27957.50 secs\n",
      "[GAN Epoch]:   131 [Cost Time]:   52319.74 secs [ETA]:   27557.73 secs\n",
      "[GAN Epoch]:   132 [Cost Time]:   52717.75 secs [ETA]:   27157.63 secs\n",
      "[GAN Epoch]:   133 [Cost Time]:   53117.46 secs [ETA]:   26758.42 secs\n",
      "[GAN Epoch]:   134 [Cost Time]:   53516.29 secs [ETA]:   26358.77 secs\n",
      "[GAN Epoch]:   135 [Cost Time]:   53915.20 secs [ETA]:   25959.17 secs\n",
      "[GAN Epoch]:   136 [Cost Time]:   54312.84 secs [ETA]:   25558.98 secs\n",
      "[GAN Epoch]:   137 [Cost Time]:   54712.95 secs [ETA]:   25159.97 secs\n",
      "[GAN Epoch]:   138 [Cost Time]:   55111.15 secs [ETA]:   24760.08 secs\n",
      "[GAN Epoch]:   139 [Cost Time]:   55511.41 secs [ETA]:   24361.12 secs\n",
      "[GAN Epoch]:   140 [Cost Time]:   55909.78 secs [ETA]:   23961.33 secs\n",
      "[GAN Epoch]:   141 [Cost Time]:   56310.15 secs [ETA]:   23562.40 secs\n",
      "[GAN Epoch]:   142 [Cost Time]:   56708.56 secs [ETA]:   23162.65 secs\n",
      "[GAN Epoch]:   143 [Cost Time]:   57108.49 secs [ETA]:   22763.52 secs\n",
      "[GAN Epoch]:   144 [Cost Time]:   57506.49 secs [ETA]:   22363.63 secs\n",
      "[GAN Epoch]:   145 [Cost Time]:   57907.24 secs [ETA]:   21964.82 secs\n",
      "[GAN Epoch]:   146 [Cost Time]:   58305.93 secs [ETA]:   21565.21 secs\n",
      "[GAN Epoch]:   147 [Cost Time]:   58704.58 secs [ETA]:   21165.60 secs\n",
      "[GAN Epoch]:   148 [Cost Time]:   59103.54 secs [ETA]:   20766.11 secs\n",
      "[GAN Epoch]:   149 [Cost Time]:   59502.26 secs [ETA]:   20366.54 secs\n",
      "[GAN Epoch]:   150 [Cost Time]:   59901.47 secs [ETA]:   19967.16 secs\n",
      "[GAN Epoch]:   151 [Cost Time]:   60300.75 secs [ETA]:   19567.79 secs\n",
      "[GAN Epoch]:   152 [Cost Time]:   60699.06 secs [ETA]:   19168.12 secs\n",
      "[GAN Epoch]:   153 [Cost Time]:   61096.90 secs [ETA]:   18768.33 secs\n",
      "[GAN Epoch]:   154 [Cost Time]:   61495.40 secs [ETA]:   18368.76 secs\n",
      "[GAN Epoch]:   155 [Cost Time]:   61894.79 secs [ETA]:   17969.46 secs\n",
      "[GAN Epoch]:   156 [Cost Time]:   62295.06 secs [ETA]:   17570.40 secs\n",
      "[GAN Epoch]:   157 [Cost Time]:   62693.70 secs [ETA]:   17170.89 secs\n",
      "[GAN Epoch]:   158 [Cost Time]:   63093.04 secs [ETA]:   16771.57 secs\n",
      "[GAN Epoch]:   159 [Cost Time]:   63491.93 secs [ETA]:   16372.13 secs\n",
      "[GAN Epoch]:   160 [Cost Time]:   63890.80 secs [ETA]:   15972.70 secs\n",
      "[GAN Epoch]:   161 [Cost Time]:   64289.59 secs [ETA]:   15573.25 secs\n",
      "[GAN Epoch]:   162 [Cost Time]:   64689.41 secs [ETA]:   15174.06 secs\n",
      "[GAN Epoch]:   163 [Cost Time]:   65087.84 secs [ETA]:   14774.54 secs\n",
      "[GAN Epoch]:   164 [Cost Time]:   65487.61 secs [ETA]:   14375.33 secs\n",
      "[GAN Epoch]:   165 [Cost Time]:   65885.77 secs [ETA]:   13975.77 secs\n",
      "[GAN Epoch]:   166 [Cost Time]:   66285.88 secs [ETA]:   13576.63 secs\n",
      "[GAN Epoch]:   167 [Cost Time]:   66684.02 secs [ETA]:   13177.08 secs\n",
      "[GAN Epoch]:   168 [Cost Time]:   67083.28 secs [ETA]:   12777.77 secs\n",
      "[GAN Epoch]:   169 [Cost Time]:   67481.53 secs [ETA]:   12378.27 secs\n",
      "[GAN Epoch]:   170 [Cost Time]:   67881.16 secs [ETA]:   11979.03 secs\n",
      "[GAN Epoch]:   171 [Cost Time]:   68279.61 secs [ETA]:   11579.58 secs\n",
      "[GAN Epoch]:   172 [Cost Time]:   68678.59 secs [ETA]:   11180.24 secs\n",
      "[GAN Epoch]:   173 [Cost Time]:   69088.22 secs [ETA]:   10782.55 secs\n",
      "[GAN Epoch]:   174 [Cost Time]:   69532.71 secs [ETA]:   10389.95 secs\n",
      "[GAN Epoch]:   175 [Cost Time]:   69970.21 secs [ETA]:    9995.74 secs\n",
      "[GAN Epoch]:   176 [Cost Time]:   70405.92 secs [ETA]:    9600.81 secs\n",
      "[GAN Epoch]:   177 [Cost Time]:   70838.62 secs [ETA]:    9205.02 secs\n",
      "[GAN Epoch]:   178 [Cost Time]:   71276.04 secs [ETA]:    8809.40 secs\n",
      "[GAN Epoch]:   179 [Cost Time]:   71707.90 secs [ETA]:    8412.66 secs\n",
      "[GAN Epoch]:   180 [Cost Time]:   72135.32 secs [ETA]:    8015.04 secs\n",
      "[GAN Epoch]:   181 [Cost Time]:   72564.32 secs [ETA]:    7617.25 secs\n",
      "[GAN Epoch]:   182 [Cost Time]:   73000.27 secs [ETA]:    7219.81 secs\n",
      "[GAN Epoch]:   183 [Cost Time]:   73428.48 secs [ETA]:    6821.22 secs\n",
      "[GAN Epoch]:   184 [Cost Time]:   73860.72 secs [ETA]:    6422.67 secs\n",
      "[GAN Epoch]:   185 [Cost Time]:   74299.14 secs [ETA]:    6024.25 secs\n",
      "[GAN Epoch]:   186 [Cost Time]:   74736.69 secs [ETA]:    5625.34 secs\n",
      "[GAN Epoch]:   187 [Cost Time]:   75171.98 secs [ETA]:    5225.86 secs\n",
      "[GAN Epoch]:   188 [Cost Time]:   75605.89 secs [ETA]:    4825.91 secs\n",
      "[GAN Epoch]:   189 [Cost Time]:   76035.39 secs [ETA]:    4425.34 secs\n",
      "[GAN Epoch]:   190 [Cost Time]:   76458.71 secs [ETA]:    4024.14 secs\n",
      "[GAN Epoch]:   191 [Cost Time]:   76889.82 secs [ETA]:    3623.08 secs\n",
      "[GAN Epoch]:   192 [Cost Time]:   77322.79 secs [ETA]:    3221.78 secs\n",
      "[GAN Epoch]:   193 [Cost Time]:   77754.74 secs [ETA]:    2820.12 secs\n",
      "[GAN Epoch]:   194 [Cost Time]:   78199.17 secs [ETA]:    2418.53 secs\n",
      "[GAN Epoch]:   195 [Cost Time]:   78621.88 secs [ETA]:    2015.95 secs\n",
      "[GAN Epoch]:   196 [Cost Time]:   79039.69 secs [ETA]:    1613.05 secs\n",
      "[GAN Epoch]:   197 [Cost Time]:   79439.23 secs [ETA]:    1209.73 secs\n",
      "[GAN Epoch]:   198 [Cost Time]:   79842.08 secs [ETA]:     806.49 secs\n",
      "[GAN Epoch]:   199 [Cost Time]:   80243.34 secs [ETA]:     403.23 secs\n",
      "[GAN Epoch]:   200 [Cost Time]:   80643.29 secs [ETA]:       0.00 secs\n"
     ]
    }
   ],
   "source": [
    "print('#########################################################################')\n",
    "print('Start Adversarial Training...')\n",
    "log.write('adversarial training...\\n')\n",
    "now_time = time.clock()\n",
    "sum_time = 0.\n",
    "for total_batch in range(TOTAL_BATCH):\n",
    "    # Train the generator for one step\n",
    "    for it in range(1):\n",
    "        samples = generator.generate(sess)\n",
    "        rewards = rollout.get_reward(sess, samples, 16, discriminator)\n",
    "        feed = {generator.x: samples, generator.rewards: rewards}\n",
    "        _ = sess.run(generator.g_updates, feed_dict=feed)\n",
    "\n",
    "    # Test\n",
    "    '''if total_batch % 5 == 0 or total_batch == TOTAL_BATCH - 1:\n",
    "        generate_samples(sess, generator, BATCH_SIZE, generated_num, eval_file)\n",
    "        likelihood_data_loader.create_batches(eval_file)\n",
    "        #print(total_batch)        \n",
    "        test_loss = target_loss(sess, target_lstm, likelihood_data_loader)\n",
    "        buffer = 'epoch:\\t' + str(total_batch) + '\\tnll:\\t' + str(test_loss) + '\\n'\n",
    "        print('total_batch: ', total_batch, 'test_loss: ', test_loss)\n",
    "        log.write(buffer)'''\n",
    "\n",
    "    # Update roll-out parameters\n",
    "    rollout.update_params()\n",
    "\n",
    "    # Train the discriminator\n",
    "    for _ in range(5): #default value=range(5)\n",
    "        adversarial_D = ( output_path + \"adversarial_gen/generator_digit_{0}.txt\").format(str(total_batch+1).zfill(3))\n",
    "        generate_samples(sess, generator, BATCH_SIZE, generated_num, adversarial_D)\n",
    "        dis_data_loader.load_train_data(positive_file, adversarial_D)\n",
    "        \n",
    "        for _ in range(3): #default value=range(3)\n",
    "            dis_data_loader.reset_pointer()\n",
    "            for it in range(dis_data_loader.num_batch):\n",
    "                if it % int(dis_data_loader.num_batch / 30) == 0:\n",
    "                    print(\"Discriminator Epoch {0:5} iteration: {1:6} / {2:6}\".format( (total_batch+1), (it+1), (dis_data_loader.num_batch) ), end=\"\\r\")\n",
    "                x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                feed = {\n",
    "                    discriminator.input_x: x_batch,\n",
    "                    discriminator.input_y: y_batch,\n",
    "                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "                }\n",
    "                _ = sess.run(discriminator.train_op, feed)\n",
    "                \n",
    "    after_time=time.clock() - now_time\n",
    "    eta_time = (after_time / (total_batch+1) )*(TOTAL_BATCH-(total_batch+1))\n",
    "    print(\"[GAN Epoch]: {0:5} [Cost Time]: {1:10.2f} secs [ETA]: {2:10.2f} secs\".format( (total_batch+1), after_time, eta_time))\n",
    "    \n",
    "log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
